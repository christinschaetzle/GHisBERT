# GHisBERT
In this repository, you'll find the code for training GHisBERT (bert_pretraining.ipynb), a BERT-based language model trained from scratch on historical data covering all attested stages of German (going back to Old High German, c. 750 CE). In addition, the repository provides code training a BERT tokenizer (bert_tokenizer.ipynb) and for fine-tuning a German BERT-base model (bert_finetuning.ipynb). This code was used for generating the results presented in the paper Beck & KÃ¶llner (2023), reference given below. 

For more information, please see the following paper:
[Will be added soon.]
