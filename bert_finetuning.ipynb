{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9fece1-0866-4d90-a936-0a02aca779b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c7de9e-99ae-43a9-9eab-0de8a92ebd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:27:53.062417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finetuning of BERT model using historical German data\n",
    "Input data: sentences extracted from Referenzkorpora zur deutschen Sprachgeschichte \n",
    "and German Data from Semeval2020 challenge on LSC\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'Christin Beck'\n",
    "__created__ = '31.05.2023'\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import *\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ebef2e-3e2c-445d-a761-dc34a61afbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file pretrained-tokenizer/config.json\n",
      "loading configuration file pretrained-tokenizer/config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = 'pretrained-tokenizer'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path)\n",
    "      \n",
    "#for fine-tuning\n",
    "model_path = 'fine-tuned-bert/german'\n",
    "#make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "        os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6af79c-7534-4e6a-86bb-29812de1041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,   637,  7065,  ...,     0,     0,     0],\n",
      "        [    2,   507,  9809,  ...,     0,     0,     0],\n",
      "        [    2,   506,  2963,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2, 19567,    17,  ...,     0,     0,     0],\n",
      "        [    2,    53,    15,  ...,     0,     0,     0],\n",
      "        [    2,   623,  1862,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "with open('train.txt', 'r') as fin:\n",
    "    train = [line.rstrip() for line in fin]\n",
    "\n",
    "with open('test.txt', 'r') as fin:\n",
    "    test = [line.rstrip() for line in fin]\n",
    "    \n",
    "train_dataset = tokenizer(train, max_length=512, padding='max_length', truncation=True, return_special_tokens_mask=True, return_tensors='pt')\n",
    "train_dataset = train_dataset['input_ids']\n",
    "\n",
    "\n",
    "test_dataset = tokenizer(test, max_length=512, padding='max_length', truncation=True, return_special_tokens_mask=True, return_tensors='pt')\n",
    "test_dataset = test_dataset['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065e3f44-c77e-4bab-8152-3abd00880263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    #device = torch.device(\"cuda\")\n",
    "    device = torch.cuda.set_device(0)\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0887ed-884a-4314-91d6-119902b56193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/scc/christin.beck/.cache/huggingface/hub/models--dbmdz--bert-base-german-cased/snapshots/56c3dce79f5d93e466f3b800d8e57cddfe13a6d4/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-german-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/scc/christin.beck/.cache/huggingface/hub/models--dbmdz--bert-base-german-cased/snapshots/56c3dce79f5d93e466f3b800d8e57cddfe13a6d4/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dbmdz/bert-base-german-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters:  109960318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Loading model from fine-tuned-bert/german/checkpoint-10000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorForLanguageModeling(tokenizer=BertTokenizerFast(name_or_path='pretrained-tokenizer', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), mlm=True, mlm_probability=0.2, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 524876\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 16400\n",
      "  Number of trainable parameters = 109960318\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 2\n",
      "  Continuing training from global step 10000\n",
      "  Will skip the first 2 epochs then the first 14400 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfcd4b120944368ac02c562887901a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16400' max='16400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16400/16400 6:04:37, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.025500</td>\n",
       "      <td>4.937214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.988900</td>\n",
       "      <td>4.896297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.943100</td>\n",
       "      <td>4.858616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.895900</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.882500</td>\n",
       "      <td>4.809449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.868900</td>\n",
       "      <td>4.795249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 131219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131219\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to fine-tuned-bert/german\n",
      "Configuration saved in fine-tuned-bert/german/config.json\n",
      "Configuration saved in fine-tuned-bert/german/generation_config.json\n",
      "Model weights saved in fine-tuned-bert/german/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "##########Initialize the model\n",
    "###Config = Europeana BERT\n",
    "model_config = BertConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=768, \n",
    "    num_hidden_layers=12, \n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512,\n",
    ")\n",
    "\n",
    "#for finetuning\n",
    "#Load pretrained model\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "print('No of parameters: ', model.num_parameters())\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2, return_tensors='pt') #randomly replaces each token with mask by 20%probability\n",
    "print(data_collator)\n",
    "    \n",
    "##########Initialize Trainer and pass arguments on\n",
    "training_args = TrainingArguments(\n",
    "    overwrite_output_dir = False,  #set to False if training continued from checkpoint\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    num_train_epochs=4, #recommendation of BERT authors for fine-tuning: 2-4\n",
    "    per_device_train_batch_size=8, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=10000,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    save_total_limit=2,           # whether you don't have much space so you let only 2 model weights saved in the disk\n",
    ")    \n",
    "    \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "    \n",
    "trainer.train(resume_from_checkpoint=\"fine-tuned-bert/german/checkpoint-10000\")\n",
    "#trainer.train()\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0400d-c039-4cee-b510-8a4162054b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1363c20-0cd6-4f8b-a41c-93f95251eaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
